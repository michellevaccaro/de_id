12/4/15

Considerable cleanup added to buildDB.py. Changed the building of the country-name from country codes (using the
cc_by_ip field) from being a separate operation that did a 'Select' over the entire db to one where a dictionary
is constructed during the building of the db itself (single pass); then saved that as a pickled dictionary rather than
adding a separate column in to the database for each record. Note that each record already has a country-name associated
with it; that name is different from the one that is given by pycountry in some instances, but only in terms of the
spelling, never in terms of the identification.

Made sure that all of the registered/viewed/explored/certified flags were integer values (explored was stored as floating
point). For number of forum posts, changed the unmarked value to '0' rather than '' or 'NA' so that the binning could be
made simpler.

The date format had changed in such a way that we were not removing the times from the date/times; changed the
splitDate() routine to account for the new format.

Added code to buildDB.py that allows generating data bases for all students; for students who had at least viewed course
content; for students who had at least explored the content; and for only students who had received a certificate.

8/26

Problem found. While the code in numeric_generalization.py creates a table entry for nforum_posts of
'9999', that value is used to mark entries where there were no posts. In the generalization code
this was replaced by the range in the table (73-9999), but in the generation of the .csv file it was replaced
by a blank (along with a blank for the mean). The correct way to deal with this is to replace the value
'9999' with '0' in both places, and have the mean from those values be '0'. Doing this meant more records
were suppressed, but the resulting files were k-anonymous.

8/24

A lot of the code has been simplified, but the failure to anonymize remains. I did build the suppression set without taking
into account the number of forum posts, and the .csv file that was generated off of that suppression set was 5-anon. So
this would indicate that the problem is in the handling of the number of forum posts, which at least narrows it down.

8/22 and 8/23

Olivia wrote to say that the de-identified sets were in fact not 5-anonymous, which I thought they
were. Trying to track down where the problem is.

Checked to make sure that none of the records that are in the full suppression set are being written into the
final CSV file. In fact, none are, so it appears that the buildDeIdentifiedCSV program is working correctly,
writing all and only those records that are not in the full suppression set.

8/20/15

Finished the first round that will de-identify the year2 data sets. Lots more suppression because of identification by
the set of classes, which is interested. Needed to deal with the floating-point appearance of the YoB and nforum posts;
the current solution is to simply slice the last two characters off (since they all end in '.0'); a more general solution
would be to take the str(int(float())) of the value. Maybe soon.

An even better improvement for the buildDeIdentifiedCSV.py program would be to create an object that was the record, and
have methods that would add the calculated fields. There could also be a method that would give the right order to the
fields when things like the mean is added. This would make more sense than the indexing that is currently happenting, which
is both obscure and error prone. This approach might actually make some sense.

8/19/15
After changing Olivia's numeric_generalization code to run as a stand-alone program, the database table that stores
the bins for numbers of forum posts (nforum_posts_bins) was not being saved, even though it used the same code as
was used to generate the bins for Year of Birth (which was being saved). Stepping through the code by hand caused the
table to be saved. Putting in print messages caused the data to be saved. But running the same code stand-alone resulted
in the table not being saved.

A classic Heisenbug; it only appears when you aren't looking. Which means that it was a timing issue.

The real problem was that the program was exiting before the implicit synchronization of the sqlite database when run
stand-alone, but this synchronization was happening in the time it took to either run the program in a python shell or
when there were print statements. The solution was to do an explicit close of the database at the end of the program,
which forces a write of any changes.

I also moved the constants YoB_binsize and nforum_post_binsize to this file, as it is the only one that uses these
constants.

6/29/15
Changing the buildcountrygeneralizer.py to work with the year two data set. The first change is
to use the cc_by_ip field in the database; this is the country code as determined by the
ip number of the most common login. While this isn't perfect as a locator, it is better than
the self-provided country codes or names, which are nearly always blank.

Moved the global geo_binsize from de_id_functions.py to this program, since this is the only
place that the global is used. It might be useful to make this into a command-line option
rather than having to edit the source code to change the bin size.

Rather than changing the .db file to include the name of the country for each student, we will
use the cc_by_ip field, which is a country code. This can be mapped to the country name by using
the pycountry library. There are other fields that proport to have the country for the student in them,
but these are self-reported and often have no content.

We then build four dictionaries:
    cc_to_countries, which maps from the cc_by_ip field to the name of a country, gotten from pycountry;
    countrydist, which maps from country code to the number of instances with that value
    country2cont, which maps from the name (not code) of a country to the region for that country; this is just
        reading in the values in a table that has already been created and stored in a file
    cont2country, which takes the country2cont table and the country code to country table and does an inverse
    mapping, allowing mapping from a region to the set of country codes


6/21/15

Extracted a main method from buildcountrygeneralizer.py so that it can be called
from another script. The main program will take the names of the files needed
to populate the in-memory structures, open them and build the structures, and then
write the country generalization table to disk. This is the first of two step for
this process (ultimately); the next step will be to extract the code that reads
and writes the files and have the processing done on in-memory structures that can
be passed from program to program by a wrapping script.

Did the same with buildFullSuppressionSet.py; this will also need to be re-factored
to get rid of writing all of the intermediate values to disk; there also needs to be
some review of the information (some of it useful, much of it for debugging) that is
currently printed out by the main routine.

Did the same with courseSetDeIdentify.py, also adding the ability to decide to use
suppression by activity or suppression by random choice to the input parameters.

5/21/15
Added a generalization dictionary for level of education. There will be two categories;
those who have finished at least a bachelor's degree (labelled 'pg') and those who have
not or do not report (labelled 'ug'). This is done with a dictionary that is wired
in to buildDeIddntifedCSV.py, which is not elegant but works.

Added constants in the de_id_functions.py file that define the bin sizes used in the
de-identification. These are YoB_binsize, nforum_post_binsize, and geo_binsize. These can
be changed independently.

The full toolchain is now done; we can generate a de-identified file using a technique
that favors generalization over suppression. The only time we use suppression is if the
set of classes uniquely identifies a student, or if the record when being written out
contains characters that can't be written to a .csv file. We have filters for class
suppression that will pick class/student combinations to suppress at random or based on
favoring suppression of the classes in which the student was the least active. We also have
ways of dynamically determining bins of a minimum size for numeric values and for
geographic locations.

The work needed now is to determine the size of the bins that will lead to a k-anonymous
data set for various values of k. For k = 5 the bins need to be surprisingly large;
currently values of 20,000 lead to more than 55k combinations of values in which 4 or
fewer students reside in the bins.

5/10/15
Time to write the routine that will write out the final, de-identified .csv file.
The fields that we want to write are
    course_id -> course_id
    de-identified student_id -> user_id
    registered -> registered

    viewed -> viewed
    explored -> explored
    certified -> certified
    final-country or region -> final_cc_cname
    Level of education -> LoE
    Year of birth -> YoB
    Gender -> gender
    grade -> grade
    start time -> start_time
    last event -> last_event
    number of events -> nevents
    number of days active -> ndays_act
    number of videos played -> nplay_video
    number of chapters -> nchapters
    number of forum posts -> nforum_posts
    roles -> roles
    incomplete flag -> ???


4/16/15
Simplified the code in edLevelDistribution to only have the list worked on contain the level of education. This
is not quite as simple as it seems, since fetchall() returns a list of tuples, so the resulting list was of
singleton tuples and so still needs to be indexed.

Looked into the data and found that YoB has as possible values both 'NA' and '' for those that hadn't entered
something. Changed buildDB.py to map YoB values of 'NA', those before 1930, and those after 2005 to the value ''.
This will replace the suppression of records with YoB before 1930, which seemed a bit extreme.

Changed the routine in edLevelDistribution that built the dictionary to be more general. It will now build
a dictionary of values/number of records with those values from any list of singleton tuples. You can also
pass in a filter function, which takes a value and can exchange that value for some other, based on whatever
metric you wish. Renamed this function from buildleveldict to builddistdict.

3/19/15
Added code to buildEquivClasses.py to allow the building of the equivalence classes from a .db file as
well as from the .csv.

3/17/15
Back to the code (although some has been generated without adding to the log). Starting by moving all of
the code needed to build the database into buildDB.py, so that it won't clutter up the de_id_functions.py.
Moved splitDate(), idGen2(), and sourceLoad() into the buildDB.py. Also making buildDB.py directly
executable, which should make things easier.

Added a check to buildCourseDict.py to make sure that the courses being added to the dictionary are listed in
a canonical order (I just used dictionary order). It turns out that this makes a difference-- the concatenation
of the classes taken by a student might differ in order only, so putting them in the same order will cut
down on some of the uniqueness.

1/21/15
Starting to build the program that will separately build the database. The thought is to build something that
will take as arguments the .csv file that contains all of the information and the name of the database to output,
and then create something that a version of De-identification.py can use to create a k-anonymous database.

1/29/15
Moved creation of the country/continent table to the buildDB.py function. Have started
to find places whereC some information was just printed out and moved into places where
it is only printed when a verbose option is picked.

1/8/15
Currently, every time that De-identification.py is run, it builds the whole sqlite
database from a .csv file that contains the information on courses. This database
contains two tables; one which is unmodified and one that is used to anonymize the 
data. 

This functionality can, and should, be split into different programs. One will create
the database, containing a (mostly) unmodified version of the csv file (that itself
contains the course and person data) and another that starts by removing the anonymized 
table, copying the original table, and then working on the anonymization.

In de_id_functions.py:
	Needed to install:
		pycountry
		pp
		pygeoip
	
	changed undeclared and unused variable c to cursor in a number of functions; assumed to be a typo
	
in De-identification.py
	Removed a bad space at line 515
	
